{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from utils import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_AREA    = '/data/cell_area.csv'\n",
    "PARQUETS_DIR = '/data/parquets'\n",
    "CURRENT_DIR  = '/home/hellscream/Documents/backendSpark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Schema"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- code: string (nullable = true)\n",
    " |-- towers: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- times: array (nullable = true)\n",
    " |    |-- element: integer (containsNull = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkSessionBase():\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession\\\n",
    "                     .builder\\\n",
    "                     .appName('Mobility')\\\n",
    "                     .getOrCreate()\n",
    "        \n",
    "        self.kdtree = None\n",
    "        \n",
    "        # Map cell_area.csv\n",
    "        self.dic_full_df = self.spark.read.format('csv').options(header='true', delimiter='\\t')\\\n",
    "                         .load(CURRENT_DIR + CELL_AREA)\\\n",
    "                         .select('*')\n",
    "                         \n",
    "        dic_full_pandas = self.dic_full_df.toPandas()\n",
    "        self.dic_full   = dic_full_pandas.to_dict('index')\n",
    "        \n",
    "        self.dic_tow_cell = {}\n",
    "        for i in self.dic_full:\n",
    "            self.dic_tow_cell[self.dic_full[i]['id']] = self.dic_full[i]['area_correlator'] \n",
    "        \n",
    "        self.dic_cell_latlon = {}\n",
    "        for i in self.dic_full:\n",
    "            self.dic_cell_latlon[self.dic_full[i]['area_correlator']] = [self.dic_full[i]['latitude'], self.dic_full[i]['longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mobility(SparkSessionBase):\n",
    "    def __init__(self, date, time_start_lower, time_start_high, time_end_lower,\n",
    "        time_end_high, time_sleep_lower='01:00', time_sleep_high='04:00'):\n",
    "        super().__init__()\n",
    "        self.date = date\n",
    "        \n",
    "    def get_mobility_at_time_interval(self, time_start, time_end):\n",
    "        # load correspondent parquet\n",
    "        \n",
    "        time_start *= 3600\n",
    "        time_end *= 3600\n",
    "        \n",
    "        df = self.spark.read.parquet(CURRENT_DIR + PARQUETS_DIR + '/' + self.date)\n",
    "\n",
    "        df = df.rdd.flatMap(lambda rows : get_range(rows, time_start, time_end))\\\n",
    "               .toDF(['code', 'towers', 'times']).select('*').filter(F.size(F.col('times')) > 0)\n",
    "        \n",
    "        mapp = self.dic_tow_cell\n",
    "        df = df.rdd.flatMap(lambda x : mapp_tow_cell(x, mapp)).toDF(['code', 'towers', 'times'])\n",
    "        \n",
    "        count_occurrences_udf = F.udf(lambda x : count_occurrences_and_normalize(x),\\\n",
    "                                      ArrayType(ArrayType(StringType())))\n",
    "        \n",
    "        df = df.select('code', count_occurrences_udf(F.col('towers')).alias('towers-count'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def build(self, users_cells_start, users_cells_end):\n",
    "        union_user_cells = user_cells_start.union(user_cells_end)\n",
    "        \n",
    "        union_user_cells = union_user_cells.groupBy('code')\\\n",
    "                                           .agg(F.collect_list('towers-count').alias('cells'), F.count('code')\\\n",
    "                                                .alias('count'))\\\n",
    "                                           .filter(F.col('count') == 2)\n",
    "        \n",
    "        rdd = union_user_cells.select('cells').rdd.flatMap(lambda x : flat_origin_destination_product(x))\n",
    "        df = rdd.toDF(['start', 'end', 'value'])\n",
    "        df = df.groupBy('start').pivot('end').agg(F.sum('value'))\n",
    "        \n",
    "        matriz_pandas = df.toPandas()\n",
    "        matriz_pandas.to_json('/home/hellscream/Documents/mobility_spark/data/2021-03-01.json', 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mobility_instance = Mobility('2021-03-01', None, None, None, None)\n",
    "\n",
    "user_cells_start = mobility_instance.get_mobility_at_time_interval(6, 10)\n",
    "user_cells_end   = mobility_instance.get_mobility_at_time_interval(16, 20)\n",
    "\n",
    "mobility_instance.build(user_cells_start, user_cells_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "print('time elapsed:' + str((time_end - time_start) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserMobility(SparkSessionBase):\n",
    "    def __init__(self, date):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.date = date\n",
    "        \n",
    "        '''\n",
    "        self.imsi_mobility = {}  # amount of km and cell_changes\n",
    "        self.users_cells_start = {}  # count tower night to set home\n",
    "        '''\n",
    "    \n",
    "    def get_users_mobility(self, start_time=25200, end_time=72000, sleep_start_time=3600, sleep_end_time=14400):\n",
    "        df = self.spark.read.parquet(CURRENT_DIR + PARQUETS_DIR + '/' + self.date)\n",
    "        \n",
    "        df = df.rdd.flatMap(lambda rows : between_ab_OR_dc(rows, (start_time, end_time), (sleep_start_time, sleep_end_time))).toDF(['code', 'cell_ids', 'times'])\n",
    "        df = df.select('*').where(F.size(df.times) > 0)\n",
    "        \n",
    "        # imsi -> cell changes & amount of distinct cell\n",
    "        distinct_cells_df = df.withColumn('distinct_cells', F.array_distinct(F.col('cell_ids'))).select('code', 'distinct_cells', F.size(F.col('distinct_cells')).alias('amount'))\n",
    "        \n",
    "        mapp = self.dic_tow_cell\n",
    "        df = df.rdd.flatMap(lambda x : mapp_tow_cell(x, mapp)).toDF(['code', 'towers', 'times'])\n",
    "                \n",
    "        # imsi -> km displacement\n",
    "        mapp = self.dic_cell_latlon\n",
    "        km_displacement_df = df.rdd.flatMap(lambda x : mapp_cell_latlon(x, mapp)).toDF(['code', 'cells', 'lat_lon'])\n",
    "        km_displacement_udf = F.udf(lambda coordinates : km_displacement(coordinates), StringType())\n",
    "        km_displacement_df = km_displacement_df.withColumn('km_displacement', km_displacement_udf(F.col('lat_lon'))).select('code', 'km_displacement')\n",
    "        \n",
    "        # imsi -> amount of distinct towers\n",
    "        distinct_towers_df = df.withColumn('distinct_towers', F.array_distinct(F.col('towers'))).select('code', 'distinct_towers', F.size(F.col('distinct_towers')).alias('amount')) \n",
    "        \n",
    "        \n",
    "        # process for establish sleeping zone\n",
    "        users_cells_start = df.rdd.flatMap(lambda x : accumulate_count(x, sleep_start_time, sleep_end_time)).toDF(['code', 'towers', 'count'])\n",
    "        users_cells_start = users_cells_start.select('*').where(F.size('towers') > 0)\n",
    "        \n",
    "        map_area_correlator_to_coord_udf = F.udf(lambda x : map_area_correlator_to_coord(x, mapp), ArrayType(ArrayType(StringType())))\n",
    "        users_cells_start = users_cells_start.withColumn('coords', map_area_correlator_to_coord_udf(F.col('towers'))).select('*')\n",
    "\n",
    "        # getting mean and home tower\n",
    "        normalize_udf = F.udf(lambda values : [val / sum(values) for val in values], ArrayType(StringType()))\n",
    "        mean_home_tower_udf = F.udf(lambda weight, coords : get_mean_home_tower(weight, coords), ArrayType(StringType()))\n",
    "        # imsi -> spleep_area\n",
    "        users_cells_start = users_cells_start.withColumn('weight', normalize_udf(F.col('count')))\\\n",
    "                                             .select('code', mean_home_tower_udf(F.col('weight'), F.col('coords')).alias('home_tower'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #return distinct_cells_df\n",
    "        #return km_displacement_df\n",
    "        #return distinct_towers_df\n",
    "        \n",
    "        return users_cells_start   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "um = UserMobility('2021-03-01')\n",
    "set1 = um.get_users_mobility()\n",
    "set1.show()\n",
    "#set1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Traces(SparkSessionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = list(map(np.array, [[5, 4], [2, 6], [13, 3], [3, 1], [10, 2], [8, 7]]))\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtree = KDTree(points)\n",
    "tar = np.array([9, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = kdtree.query(tar, 1)\n",
    "res = kdtree.data[q[1]]\n",
    "res = list(res)\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {0: {'start' : 'A', 'k1' : 0, 'k2' : 5}, 1: {'start' : 'B', 'k3' : 10}}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df.toPandas()\n",
    "r1 = r.to_json('index')\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_format(jobj):\n",
    "    m = {}\n",
    "    for i in jobj:\n",
    "        m[jobj[i]['start']] = jobj[i]\n",
    "        del m[jobj[i]['start']]['start']\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\"0\":{\"start\":\"B\",\"B\":0,\"C\":8.0},\"1\":{\"start\":\"C\",\"A\":0,\"C\":15.0},\"2\":{\"start\":\"A\",\"B\":10.0,\"C\":0}}\n",
    "\n",
    "d2 = {\"start\":{\"0\":\"B\",\"1\":\"C\",\"2\":\"A\"},\"B\":{\"0\":0,\"1\":0,\"2\":10.0},\"C\":{\"0\":8.0,\"1\":15.0,\"2\":0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B     C     A\n",
       "B  0.0   NaN  10.0\n",
       "C  8.0  15.0   0.0\n",
       "A  NaN   0.0   NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = fix_json_format(d1)\n",
    "df = pd.DataFrame(m)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': {'B': 0.0, 'C': 8.0, 'A': nan},\n",
       " 'C': {'B': nan, 'C': 15.0, 'A': 0.0},\n",
       " 'A': {'B': 10.0, 'C': 0.0, 'A': nan}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B     C    A\n",
       "B   0.0   8.0  NaN\n",
       "C   NaN  15.0  0.0\n",
       "A  10.0   0.0  NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df.transpose()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': {'B': 0.0, 'C': nan, 'A': 10.0},\n",
       " 'C': {'B': 8.0, 'C': 15.0, 'A': 0.0},\n",
       " 'A': {'B': nan, 'C': 0.0, 'A': nan}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"0\":{\"start\":\"B\",\"B\":0,\"C\":8},\"1\":{\"start\":\"C\",\"B\":0,\"C\":15},\"2\":{\"start\":\"A\",\"B\":10,\"C\":0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': {'B': 0, 'C': 8}, 'C': {'B': 0, 'C': 15}, 'A': {'B': 10, 'C': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(fix_json_format(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "    df = spark_session.createDataFrame([Row(1, [['a', '2'], ['b', '2']]), Row(2, [['a', '4']])], ['col1', 'col2'])\n",
    "    df.show()\n",
    "\n",
    "    df = df.rdd.flatMap(lambda row : ((tow, val) for tow, val in row[1]) ).toDF(['tower', 'val'])\n",
    "    res = df.groupBy('tower').agg(F.sum('val').alias('sum')).toPandas().to_dict(orient='list')\n",
    "    x = dict(zip(res['tower'], res['sum']))\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
